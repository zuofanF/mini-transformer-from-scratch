import torch
import torch.nn as nn
from attention import MultiHeadAttention


class PositionwiseFeedForward(nn.Module):
    """
    Position-wise Feed-Forward Network
    
    FFN(x) = max(0, x Wâ‚ + bâ‚) Wâ‚‚ + bâ‚‚
    """
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        Args:
            d_model: æ¨¡å‹ç»´åº¦ (512)
            d_ff: FFN ä¸­é—´å±‚ç»´åº¦ (2048)
            dropout: dropout æ¯”ä¾‹
        """
        super().__init__()
        
        # TODO 1: å®šä¹‰ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ (d_model â†’ d_ff)
        self.linear1 = nn.Linear(d_model, d_ff)
        
        # TODO 2: å®šä¹‰ç¬¬äºŒä¸ªçº¿æ€§å±‚ (d_ff â†’ d_model)
        self.linear2 = nn.Linear(d_ff, d_model)
        
        # TODO 3: å®šä¹‰ dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)
        Returns:
            (batch, seq_len, d_model)
        """
        # TODO 4: x â†’ linear1 â†’ ReLU â†’ dropout â†’ linear2 â†’ dropout
        # æç¤º: ä½¿ç”¨ torch.relu() æˆ– F.relu()
        x = self.linear1(x)
        x = torch.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        x = self.dropout(x)
        return x


class EncoderLayer(nn.Module):
    """
    ä¸€å±‚ Encoder
    
    åŒ…å«:
    1. Multi-Head Self-Attention + Add & Norm
    2. Feed-Forward Network + Add & Norm
    """
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        """
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            d_ff: FFN ä¸­é—´å±‚ç»´åº¦
            dropout: dropout æ¯”ä¾‹
        """
        super().__init__()
        
        # TODO 5: å®šä¹‰ Multi-Head Attention
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        
        # TODO 6: å®šä¹‰ Feed-Forward Network
        self.ffn = PositionwiseFeedForward(d_model, num_heads)
        
        # TODO 7: å®šä¹‰ä¸¤ä¸ª LayerNorm (ä¸€ä¸ªç»™ attentionï¼Œä¸€ä¸ªç»™ ffn)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # TODO 8: å®šä¹‰ä¸¤ä¸ª Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        """
        Args:
            x: (batch, seq_len, d_model)
            mask: (batch, seq_len, seq_len)
        Returns:
            (batch, seq_len, d_model)
        """
        # TODO 9: Multi-Head Attention + Add & Norm
        # æ®‹å·®è¿æ¥: x = x + dropout(attention(x))
        # LayerNorm: x = norm(x)
        
        # Step 1: Self-Attention
        attn_output, _ = self.self_attn(x, x, x, mask)
        
        # Step 2: Add (residual) & Dropout
        x = x + self.dropout1(attn_output)
        
        # Step 3: Norm
        x = self.norm1(x)
        
        # TODO 10: Feed-Forward + Add & Norm
        # Step 1: FFN
        ffn_output = self.ffn(x)
        
        # Step 2: Add (residual) & Dropout
        x = x + self.dropout2(ffn_output)
        
        # Step 3: Norm
        x = self.norm2(x)
        
        return x


class Encoder(nn.Module):
    """
    å®Œæ•´çš„ Encoder (å †å å¤šå±‚ EncoderLayer)
    """
    
    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):
        """
        Args:
            num_layers: Encoder å±‚æ•° (æ¯”å¦‚ 6)
            d_model: æ¨¡å‹ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            d_ff: FFN ä¸­é—´å±‚ç»´åº¦
            dropout: dropout æ¯”ä¾‹
        """
        super().__init__()
        
        # TODO 11: åˆ›å»º num_layers ä¸ª EncoderLayer
        # æç¤º: ä½¿ç”¨ nn.ModuleList
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
    def forward(self, x, mask=None):
        """
        Args:
            x: (batch, seq_len, d_model)
            mask: (batch, seq_len, seq_len)
        Returns:
            (batch, seq_len, d_model)
        """
        # TODO 12: ä¾æ¬¡é€šè¿‡æ¯ä¸€å±‚
        for layer in self.layers:
            x = layer(x, mask)
        return x


# ===== æµ‹è¯•ä»£ç  =====
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª Day3 æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®å‚æ•°
    batch_size = 2
    seq_len = 10
    d_model = 512
    num_heads = 8
    d_ff = 2048
    num_layers = 6
    
    # åˆ›å»ºéšæœºè¾“å…¥
    torch.manual_seed(42)
    x = torch.randn(batch_size, seq_len, d_model)
    
    # æµ‹è¯• FFN
    print("\nâœ… PositionwiseFeedForward æµ‹è¯•")
    ffn = PositionwiseFeedForward(d_model, d_ff)
    ffn_output = ffn(x)
    print(f"Input shape: {x.shape}")
    print(f"FFN output shape: {ffn_output.shape}")
    
    # æµ‹è¯• EncoderLayer
    print("\nâœ… EncoderLayer æµ‹è¯•")
    encoder_layer = EncoderLayer(d_model, num_heads, d_ff)
    layer_output = encoder_layer(x)
    print(f"Input shape: {x.shape}")
    print(f"EncoderLayer output shape: {layer_output.shape}")
    
    # æµ‹è¯•å®Œæ•´ Encoder
    print("\nâœ… Encoder æµ‹è¯•")
    encoder = Encoder(num_layers, d_model, num_heads, d_ff)
    encoder_output = encoder(x)
    print(f"Input shape: {x.shape}")
    print(f"Encoder output shape: {encoder_output.shape}")
    print(f"Number of layers: {num_layers}")
    
    print("=" * 60)