import torch
import torch.nn as nn
from attention import MultiHeadAttention
from modules import PositionwiseFeedForward


class DecoderLayer(nn.Module):
    """
    ä¸€å±‚ Decoder
    
    åŒ…å«:
    1. Masked Self-Attention + Add & Norm
    2. Cross-Attention + Add & Norm
    3. Feed-Forward + Add & Norm
    """
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # TODO 1: å®šä¹‰ Masked Self-Attention
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        
        # TODO 2: å®šä¹‰ Cross-Attention (Encoder-Decoder Attention)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        
        # TODO 3: å®šä¹‰ Feed-Forward
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        # TODO 4: å®šä¹‰ 3 ä¸ª LayerNorm
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        # TODO 5: å®šä¹‰ 3 ä¸ª Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)
        
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        Args:
            x: Decoder è¾“å…¥ (batch, tgt_seq_len, d_model)
            encoder_output: Encoder è¾“å‡º (batch, src_seq_len, d_model)
            src_mask: Encoder çš„ padding mask (batch, 1, src_seq_len)
            tgt_mask: Decoder çš„ subsequent mask (tgt_seq_len, tgt_seq_len)
        Returns:
            (batch, tgt_seq_len, d_model)
        """
        # TODO 6: Masked Self-Attention + Add & Norm
        # Q, K, V éƒ½æ¥è‡ª xï¼Œä½¿ç”¨ tgt_mask
        attn_output, _ = self.self_attn(x, x, x, tgt_mask)
        x = x + self.dropout1(attn_output)
        x = self.norm1(x)
        
        # TODO 7: Cross-Attention + Add & Norm
        # Q æ¥è‡ª x (decoder)ï¼ŒK å’Œ V æ¥è‡ª encoder_outputï¼Œä½¿ç”¨ src_mask
        cross_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = x + self.dropout2(cross_output)
        x = self.norm2(x)
        
        # TODO 8: Feed-Forward + Add & Norm
        ffn_output = self.ffn(x)
        x = x + self.dropout3(ffn_output)
        x = self.norm3(x)
        
        return x


class Decoder(nn.Module):
    """
    å®Œæ•´çš„ Decoder (å †å å¤šå±‚ DecoderLayer)
    """
    
    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # TODO 9: åˆ›å»º num_layers ä¸ª DecoderLayer
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        Args:
            x: (batch, tgt_seq_len, d_model)
            encoder_output: (batch, src_seq_len, d_model)
            src_mask: (batch, 1, src_seq_len)
            tgt_mask: (tgt_seq_len, tgt_seq_len)
        Returns:
            (batch, tgt_seq_len, d_model)
        """
        # TODO 10: ä¾æ¬¡é€šè¿‡æ¯ä¸€å±‚
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return x


class Transformer(nn.Module):
    """
    å®Œæ•´çš„ Transformer = Encoder + Decoder
    """
    
    def __init__(
        self,
        src_vocab_size,      # æºè¯­è¨€è¯æ±‡è¡¨å¤§å°
        tgt_vocab_size,      # ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°
        d_model=512,
        num_heads=8,
        num_layers=6,
        d_ff=2048,
        dropout=0.1,
        max_seq_len=5000
    ):
        super().__init__()
        
        from modules import Encoder
        
        # TODO 11: å®šä¹‰ Encoder
        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, dropout)
        
        # TODO 12: å®šä¹‰ Decoder
        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, dropout)
        
        # TODO 13: å®šä¹‰æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„ Embedding
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # TODO 14: å®šä¹‰ä½ç½®ç¼–ç  (Positional Encoding)
        # å…ˆç”¨ä¸€ä¸ªç®€å•çš„å¯å­¦ä¹ çš„ä½ç½®ç¼–ç 
        self.src_pos_embedding = nn.Embedding(max_seq_len, d_model)
        self.tgt_pos_embedding = nn.Embedding(max_seq_len, d_model)
        
        # TODO 15: å®šä¹‰æœ€åçš„çº¿æ€§å±‚ (d_model â†’ tgt_vocab_size)
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """
        Args:
            src: æºåºåˆ— (batch, src_seq_len)
            tgt: ç›®æ ‡åºåˆ— (batch, tgt_seq_len)
            src_mask: æºåºåˆ— mask
            tgt_mask: ç›®æ ‡åºåˆ— mask
        Returns:
            (batch, tgt_seq_len, tgt_vocab_size)
        """
        batch_size = src.size(0)
        src_seq_len = src.size(1)
        tgt_seq_len = tgt.size(1)
        
        # TODO 16: æºåºåˆ—çš„ embedding + ä½ç½®ç¼–ç 
        src_pos = torch.arange(0, src_seq_len).unsqueeze(0).repeat(batch_size, 1).to(src.device)
        src_embedded = self.dropout(
            self.src_embedding(src) + self.src_pos_embedding(src_pos)
        )
        
        # TODO 17: ç›®æ ‡åºåˆ—çš„ embedding + ä½ç½®ç¼–ç 
        tgt_pos = torch.arange(0, tgt_seq_len).unsqueeze(0).repeat(batch_size, 1).to(tgt.device)
        tgt_embedded = self.dropout(
            self.tgt_embedding(tgt) + self.tgt_pos_embedding(tgt_pos)
        )
        
        # TODO 18: é€šè¿‡ Encoder
        encoder_output = self.encoder(src_embedded, src_mask)
        
        # TODO 19: é€šè¿‡ Decoder
        decoder_output = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)
        
        # TODO 20: é€šè¿‡æœ€åçš„çº¿æ€§å±‚
        output = self.fc_out(decoder_output)
        
        return output


# ===== æµ‹è¯•ä»£ç  =====
if __name__ == "__main__":
    from attention import generate_subsequent_mask
    
    print("=" * 60)
    print("ğŸ§ª Day4 æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®å‚æ•°
    batch_size = 2
    src_seq_len = 10
    tgt_seq_len = 8
    src_vocab_size = 1000
    tgt_vocab_size = 800
    d_model = 512
    num_heads = 8
    num_layers = 6
    d_ff = 2048
    
    # åˆ›å»ºéšæœºè¾“å…¥
    torch.manual_seed(42)
    src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))
    tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))
    
    # åˆ›å»º mask
    tgt_mask = generate_subsequent_mask(tgt_seq_len)
    
    # æµ‹è¯• DecoderLayer
    print("\nâœ… DecoderLayer æµ‹è¯•")
    decoder_layer = DecoderLayer(d_model, num_heads, d_ff)
    x = torch.randn(batch_size, tgt_seq_len, d_model)
    encoder_output = torch.randn(batch_size, src_seq_len, d_model)
    layer_output = decoder_layer(x, encoder_output, None, tgt_mask)
    print(f"Input shape: {x.shape}")
    print(f"Encoder output shape: {encoder_output.shape}")
    print(f"DecoderLayer output shape: {layer_output.shape}")
    
    # æµ‹è¯•å®Œæ•´ Decoder
    print("\nâœ… Decoder æµ‹è¯•")
    decoder = Decoder(num_layers, d_model, num_heads, d_ff)
    decoder_output = decoder(x, encoder_output, None, tgt_mask)
    print(f"Decoder output shape: {decoder_output.shape}")
    
    # æµ‹è¯•å®Œæ•´ Transformer
    print("\nâœ… Transformer æµ‹è¯•")
    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff)
    output = model(src, tgt, None, tgt_mask)
    print(f"Source shape: {src.shape}")
    print(f"Target shape: {tgt.shape}")
    print(f"Transformer output shape: {output.shape}")
    print(f"Expected shape: (batch={batch_size}, tgt_seq_len={tgt_seq_len}, tgt_vocab={tgt_vocab_size})")
    
    print("=" * 60)