import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import urllib.request
import zipfile
import os

class Vocabulary:
    """æ„å»ºè¯æ±‡è¡¨"""
    
    def __init__(self, is_japanese=False):
        self.word2idx = {"<pad>": 0, "<sos>": 1, "<eos>": 2, "<unk>": 3}
        self.idx2word = {0: "<pad>", 1: "<sos>", 2: "<eos>", 3: "<unk>"}
        self.word_count = Counter()
        self.is_japanese = is_japanese  # æ–°å¢ï¼šæ ‡è®°æ˜¯å¦æ˜¯æ—¥è¯­
        
    def add_sentence(self, sentence):
        """æ·»åŠ å¥å­åˆ°è¯æ±‡è¡¨"""
        if self.is_japanese:
            # æ—¥è¯­ï¼šå­—ç¬¦çº§åˆ«
            for char in sentence:
                if char.strip():  # è·³è¿‡ç©ºæ ¼
                    self.word_count[char] += 1
        else:
            # è‹±è¯­ï¼šè¯çº§åˆ«
            for word in sentence.split():
                self.word_count[word] += 1
            
    def build_vocab(self, min_count=1):  # æ”¹æˆ min_count=1
        """æ„å»ºè¯æ±‡è¡¨"""
        idx = 4
        for word, count in self.word_count.items():
            if count >= min_count:
                self.word2idx[word] = idx
                self.idx2word[idx] = word
                idx += 1
                
    def __len__(self):
        return len(self.word2idx)
    
    def encode(self, sentence, max_len=None):
        """å°†å¥å­è½¬æ¢ä¸º ID åºåˆ—"""
        if self.is_japanese:
            # æ—¥è¯­ï¼šå­—ç¬¦çº§åˆ«
            tokens = [self.word2idx.get(char, 3) for char in sentence if char.strip()]
        else:
            # è‹±è¯­ï¼šè¯çº§åˆ«
            tokens = [self.word2idx.get(word, 3) for word in sentence.split()]
        
        # æ·»åŠ  <sos> å’Œ <eos>
        tokens = [1] + tokens + [2]
        
        # Padding
        if max_len:
            if len(tokens) < max_len:
                tokens += [0] * (max_len - len(tokens))
            else:
                tokens = tokens[:max_len]
                
        return tokens
    
    def decode(self, indices):
        """å°† ID åºåˆ—è½¬æ¢å›å¥å­"""
        words = []
        for idx in indices:
            if idx == 2:  # <eos>
                break
            if idx not in [0, 1]:  # è·³è¿‡ <pad> å’Œ <sos>
                words.append(self.idx2word.get(idx, "<unk>"))
        
        # æ—¥è¯­ä¸éœ€è¦ç©ºæ ¼
        if self.is_japanese:
            return "".join(words)
        else:
            return " ".join(words)


class TatoebaDataset(Dataset):
    """Tatoeba æ•°æ®é›†"""
    
    def __init__(self, pairs, src_vocab, tgt_vocab, max_len=50):
        self.pairs = pairs
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_len = max_len
        
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        src_text, tgt_text = self.pairs[idx]
        
        src_ids = self.src_vocab.encode(src_text, self.max_len)
        tgt_ids = self.tgt_vocab.encode(tgt_text, self.max_len)
        
        return torch.tensor(src_ids), torch.tensor(tgt_ids)


def download_tatoeba():
    """ä¸‹è½½ Tatoeba EN-JP æ•°æ®é›†"""
    url = "https://www.manythings.org/anki/jpn-eng.zip"
    zip_path = "jpn-eng.zip"
    
    if not os.path.exists("jpn.txt"):
        print("ğŸ“¥ ä¸‹è½½ Tatoeba EN-JP æ•°æ®é›†...")
        
        # æ·»åŠ  User-Agent å¤´é¿å… 406 é”™è¯¯
        req = urllib.request.Request(
            url,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )
        
        try:
            with urllib.request.urlopen(req) as response:
                with open(zip_path, 'wb') as out_file:
                    out_file.write(response.read())
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(".")
            
            os.remove(zip_path)
            print("âœ… ä¸‹è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            print("\nğŸ’¡ å¤‡ç”¨æ–¹æ¡ˆï¼šè¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†")
            print("1. è®¿é—®ï¼šhttps://www.manythings.org/anki/jpn-eng.zip")
            print("2. è§£å‹å¾—åˆ° jpn.txt")
            print("3. æŠŠ jpn.txt æ”¾åˆ°å½“å‰ç›®å½•")
            raise
    else:
        print("âœ… æ•°æ®é›†å·²å­˜åœ¨")


def load_data(num_samples=10000):
    """åŠ è½½å¹¶å¤„ç†æ•°æ®"""
    download_tatoeba()
    
    print(f"ğŸ“– åŠ è½½å‰ {num_samples} ä¸ªæ ·æœ¬...")
    
    pairs = []
    with open("jpn.txt", "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i >= num_samples:
                break
            parts = line.strip().split("\t")
            if len(parts) >= 2:
                eng = parts[0].lower()
                jpn = parts[1]
                pairs.append((eng, jpn))
    
    print(f"âœ… åŠ è½½äº† {len(pairs)} ä¸ªå¥å­å¯¹")
    
    # æ„å»ºè¯æ±‡è¡¨
    print("ğŸ”¨ æ„å»ºè¯æ±‡è¡¨...")
    src_vocab = Vocabulary(is_japanese=False)  # è‹±è¯­
    tgt_vocab = Vocabulary(is_japanese=True)   # æ—¥è¯­
    
    for eng, jpn in pairs:
        src_vocab.add_sentence(eng)
        tgt_vocab.add_sentence(jpn)
    
    src_vocab.build_vocab(min_count=2)
    tgt_vocab.build_vocab(min_count=1)  # æ—¥è¯­ç”¨ min_count=1
    
    print(f"âœ… è‹±è¯­è¯æ±‡é‡: {len(src_vocab)}")
    print(f"âœ… æ—¥è¯­è¯æ±‡é‡: {len(tgt_vocab)}")
    
    return pairs, src_vocab, tgt_vocab


def get_dataloaders(batch_size=32, num_samples=10000):
    """åˆ›å»º DataLoader"""
    pairs, src_vocab, tgt_vocab = load_data(num_samples)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    split = int(0.9 * len(pairs))
    train_pairs = pairs[:split]
    val_pairs = pairs[split:]
    
    train_dataset = TatoebaDataset(train_pairs, src_vocab, tgt_vocab)
    val_dataset = TatoebaDataset(val_pairs, src_vocab, tgt_vocab)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, val_loader, src_vocab, tgt_vocab


# æµ‹è¯•
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½")
    print("=" * 60)
    
    train_loader, val_loader, src_vocab, tgt_vocab = get_dataloaders(batch_size=4, num_samples=100)
    
    # æ‰“å°ä¸€ä¸ª batch
    for src, tgt in train_loader:
        print(f"\nSource shape: {src.shape}")
        print(f"Target shape: {tgt.shape}")
        
        print(f"\nç¤ºä¾‹å¥å­:")
        print(f"English: {src_vocab.decode(src[0].tolist())}")
        print(f"Japanese: {tgt_vocab.decode(tgt[0].tolist())}")
        break
    
    print("=" * 60)
