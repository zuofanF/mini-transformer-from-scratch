import torch
import torch.nn as nn
import math

class ScaledDotProductAttention(nn.Module):
    """
    Scaled Dot-Product Attention
    
    Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V
    """
    
    def __init__(self):
        super().__init__()
        
    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q: (batch_size, seq_len_q, d_k)
            K: (batch_size, seq_len_k, d_k)
            V: (batch_size, seq_len_v, d_v)  # é€šå¸¸ d_v = d_k
            mask: (batch_size, seq_len_q, seq_len_k) æˆ– broadcastable shape
        
        Returns:
            output: (batch_size, seq_len_q, d_v)
            attn: (batch_size, seq_len_q, seq_len_k)  # attention weights
        """
        
        # TODO 1: è®¡ç®— d_k (key çš„ç»´åº¦)
        d_k = Q.shape[-1]
        
        # TODO 2: è®¡ç®— scores = Q @ K^T / sqrt(d_k)
        # æç¤º: ä½¿ç”¨ torch.matmul å’Œ .transpose()
        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(d_k)
        
        # TODO 3: å¦‚æœæœ‰ maskï¼Œå°† mask ä½ç½®çš„ scores è®¾ä¸º -1e9
        if mask is not None:
            # ç¡®ä¿ mask çš„ç»´åº¦æ­£ç¡®
            if mask.dim() == 2:
                # (seq, seq) â†’ (1, 1, seq, seq)
                mask = mask.unsqueeze(0).unsqueeze(0)
            elif mask.dim() == 3:
                # (batch, seq, seq) â†’ (batch, 1, seq, seq)
                mask = mask.unsqueeze(1)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # TODO 4: å¯¹ scores åº”ç”¨ softmax (åœ¨æœ€åä¸€ä¸ªç»´åº¦)
        attn = torch.softmax(scores, dim=-1)
        
        # TODO 5: è®¡ç®— output = attn @ V
        output = torch.matmul(attn, V)
        
        return output, attn





# ===== Day2 æ–°å¢ =====
class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention
    
    MultiHead(Q, K, V) = Concat(headâ‚, ..., headâ‚•) W_o
    where headáµ¢ = Attention(Q W_Qâ±, K W_Kâ±, V W_Vâ±)
    """
    
    def __init__(self, d_model, num_heads):
        """
        Args:
            d_model: æ¨¡å‹ç»´åº¦ï¼ˆæ¯”å¦‚ 512ï¼‰
            num_heads: æ³¨æ„åŠ›å¤´æ•°ï¼ˆæ¯”å¦‚ 8ï¼‰
        """
        super().__init__()
        
        # TODO 1: æ£€æŸ¥ d_model èƒ½å¦è¢« num_heads æ•´é™¤
        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"
        
        self.d_model = d_model
        self.num_heads = num_heads
        # TODO 2: è®¡ç®—æ¯ä¸ªå¤´çš„ç»´åº¦ d_k
        self.d_k = d_model // num_heads
        
        # TODO 3: å®šä¹‰ Q, K, V çš„çº¿æ€§å˜æ¢å±‚
        # æç¤º: è¾“å…¥ d_model, è¾“å‡º d_model (å› ä¸ºè¦åˆ†æˆ num_heads ä¸ª d_k)
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        
        # TODO 4: å®šä¹‰è¾“å‡ºçº¿æ€§å±‚ W_o
        self.W_O = nn.Linear(d_model, d_model)
        
        # å¤ç”¨ Day1 çš„ attention
        self.attention = ScaledDotProductAttention()
        
    def split_heads(self, x):
        """
        å°†è¾“å…¥æ‹†åˆ†æˆå¤šä¸ªå¤´
        
        Args:
            x: (batch, seq_len, d_model)
        Returns:
            (batch, num_heads, seq_len, d_k)
        """
        batch_size, seq_len, d_model = x.size()
        
        # TODO 5: reshape æˆ (batch, seq_len, num_heads, d_k)
        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)
        
        # TODO 6: è½¬ç½®æˆ (batch, num_heads, seq_len, d_k)
        # æç¤º: ä½¿ç”¨ .transpose(1, 2)
        return x.transpose(1, 2)
        
    def combine_heads(self, x):
        """
        åˆå¹¶å¤šä¸ªå¤´
        
        Args:
            x: (batch, num_heads, seq_len, d_k)
        Returns:
            (batch, seq_len, d_model)
        """
        batch_size, num_heads, seq_len, d_k = x.size()
        
        # TODO 7: è½¬ç½®å› (batch, seq_len, num_heads, d_k)
        x = x.transpose(1, 2)
        
        # TODO 8: reshape æˆ (batch, seq_len, d_model)
        return x.contiguous().view(batch_size, seq_len, self.d_model)
        
    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q, K, V: (batch, seq_len, d_model)
            mask: (batch, seq_len, seq_len) æˆ– broadcastable
        Returns:
            output: (batch, seq_len, d_model)
            attn: (batch, num_heads, seq_len, seq_len)
        """
        batch_size = Q.size(0)
        
        # TODO 9: é€šè¿‡çº¿æ€§å±‚å˜æ¢ Q, K, V
        Q = self.W_Q(Q)  # (batch, seq_len, d_model)
        K = self.W_K(K)
        V = self.W_V(V)
        
        # TODO 10: æ‹†åˆ†æˆå¤šä¸ªå¤´
        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # TODO 11: è°ƒæ•´ mask çš„ç»´åº¦ä»¥é€‚é…å¤šå¤´
        # æç¤º: mask éœ€è¦ä» (batch, seq, seq) å˜æˆ (batch, 1, seq, seq)
        #       è¿™æ ·å¯ä»¥ broadcast åˆ°æ‰€æœ‰ heads
        # if mask is not None:
        #     mask = mask.unsqueeze(1)  # (batch, 1, seq_len, seq_len)
        
        # TODO 12: åº”ç”¨ attention
        output, attn = self.attention(Q, K, V, mask)
        # output: (batch, num_heads, seq_len, d_k)
        # attn: (batch, num_heads, seq_len, seq_len)
        
        # TODO 13: åˆå¹¶å¤šä¸ªå¤´
        output = self.combine_heads(output)  # (batch, seq_len, d_model)
        
        # TODO 14: é€šè¿‡è¾“å‡ºçº¿æ€§å±‚
        output = self.W_O(output)
        
        return output, attn


# ===== Mask ç”Ÿæˆå‡½æ•° =====
def generate_padding_mask(seq, pad_idx=0):
    """
    ç”Ÿæˆ Padding Mask
    
    Args:
        seq: (batch, seq_len) - token IDs
        pad_idx: padding token çš„ ID (é»˜è®¤ 0)
    
    Returns:
        mask: (batch, 1, seq_len) - 1 è¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼Œ0 è¡¨ç¤º padding
    
    Example:
        seq = [[1, 2, 3, 0, 0],
               [1, 2, 0, 0, 0]]
        
        mask = [[1, 1, 1, 0, 0],
                [1, 1, 0, 0, 0]]
    """
    # TODO 15: ç”Ÿæˆ mask (seq != pad_idx)
    # æç¤º: (seq != pad_idx) ä¼šè¿”å› True/Falseï¼Œéœ€è¦è½¬æˆ int
    mask = (seq != pad_idx).int()
    return mask.unsqueeze(1)  # (batch, 1, seq_len)


def generate_subsequent_mask(size):
    """
    ç”Ÿæˆ Subsequent (Causal) Mask - é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯
    
    Args:
        size: sequence length
    
    Returns:
        mask: (size, size) - ä¸‹ä¸‰è§’çŸ©é˜µ
    
    Example:
        size = 4
        mask = [[1, 0, 0, 0],
                [1, 1, 0, 0],
                [1, 1, 1, 0],
                [1, 1, 1, 1]]
    """
    # TODO 16: ç”Ÿæˆä¸‹ä¸‰è§’çŸ©é˜µ
    # æç¤º: ä½¿ç”¨ torch.tril(torch.ones(size, size))
    mask = torch.tril(torch.ones(size, size))
    return mask


# ===== æµ‹è¯•ä»£ç  =====
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª Day2 æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®å‚æ•°
    batch_size = 2
    seq_len = 5
    d_model = 512
    num_heads = 8
    
    # åˆ›å»ºéšæœºè¾“å…¥
    torch.manual_seed(42)
    Q = torch.randn(batch_size, seq_len, d_model)
    K = torch.randn(batch_size, seq_len, d_model)
    V = torch.randn(batch_size, seq_len, d_model)
    
    # åˆå§‹åŒ– Multi-Head Attention
    mha = MultiHeadAttention(d_model, num_heads)
    
    # Forward pass
    output, attn = mha(Q, K, V)
    
    print(f"\nâœ… MultiHeadAttention æµ‹è¯•")
    print(f"Input shape: {Q.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Attention shape: {attn.shape}")
    print(f"d_k per head: {mha.d_k}")
    
    # æµ‹è¯• Padding Mask
    print(f"\nâœ… Padding Mask æµ‹è¯•")
    seq = torch.tensor([[1, 2, 3, 0, 0],
                        [1, 2, 0, 0, 0]])
    pad_mask = generate_padding_mask(seq, pad_idx=0)
    print(f"Sequence:\n{seq}")
    print(f"Padding mask shape: {pad_mask.shape}")
    print(f"Padding mask:\n{pad_mask.squeeze(1)}")
    
    # æµ‹è¯• Subsequent Mask
    print(f"\nâœ… Subsequent Mask æµ‹è¯•")
    sub_mask = generate_subsequent_mask(4)
    print(f"Subsequent mask shape: {sub_mask.shape}")
    print(f"Subsequent mask:\n{sub_mask}")
    
    print("=" * 60)





# # ===== Day 1 æµ‹è¯•ä»£ç  =====
# if __name__ == "__main__":
#     # è®¾ç½®éšæœºç§å­
#     torch.manual_seed(42)
    
#     # åˆ›å»ºæµ‹è¯•æ•°æ®
#     batch_size = 2
#     seq_len = 4
#     d_k = 8
    
#     Q = torch.randn(batch_size, seq_len, d_k)
#     K = torch.randn(batch_size, seq_len, d_k)
#     V = torch.randn(batch_size, seq_len, d_k)
    
#     # åˆå§‹åŒ– attention
#     attention = ScaledDotProductAttention()
    
#     # Forward pass
#     output, attn_weights = attention(Q, K, V)
    
#     # æ‰“å°ç»“æœ
#     print("=" * 50)
#     print("âœ… Day1 æµ‹è¯•ç»“æœ")
#     print("=" * 50)
#     print(f"Q shape: {Q.shape}")
#     print(f"K shape: {K.shape}")
#     print(f"V shape: {V.shape}")
#     print(f"\nOutput shape: {output.shape}")
#     print(f"Attention weights shape: {attn_weights.shape}")
#     print(f"\nAttention weights sum (åº”è¯¥â‰ˆ1.0): {attn_weights[0, 0].sum().item():.4f}")
#     print("=" * 50)