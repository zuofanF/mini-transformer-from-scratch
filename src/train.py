import torch
import torch.nn as nn
from torch.optim import Adam
from tqdm import tqdm
import time
import matplotlib.pyplot as plt  # æ–°å¢

from transformer import Transformer
from data_tatoeba import get_dataloaders
from attention import generate_subsequent_mask


def train_epoch(model, dataloader, optimizer, criterion, device):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    
    pbar = tqdm(dataloader, desc="Training")
    for src, tgt in pbar:
        src = src.to(device)
        tgt = tgt.to(device)
        
        # å‡†å¤‡ decoder è¾“å…¥å’Œç›®æ ‡
        tgt_input = tgt[:, :-1]  # å»æ‰æœ€åä¸€ä¸ªè¯
        tgt_output = tgt[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªè¯ (<sos>)
        
        # ç”Ÿæˆ mask
        tgt_mask = generate_subsequent_mask(tgt_input.size(1)).to(device)
        
        # Forward
        optimizer.zero_grad()
        output = model(src, tgt_input, src_mask=None, tgt_mask=tgt_mask)
        
        # è®¡ç®— loss
        output = output.reshape(-1, output.size(-1))  # (batch*seq, vocab)
        tgt_output = tgt_output.reshape(-1)           # (batch*seq)
        
        loss = criterion(output, tgt_output)
        
        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
        optimizer.step()
        
        total_loss += loss.item()
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return total_loss / len(dataloader)


def evaluate(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for src, tgt in dataloader:
            src = src.to(device)
            tgt = tgt.to(device)
            
            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]
            
            tgt_mask = generate_subsequent_mask(tgt_input.size(1)).to(device)
            
            output = model(src, tgt_input, src_mask=None, tgt_mask=tgt_mask)
            
            output = output.reshape(-1, output.size(-1))
            tgt_output = tgt_output.reshape(-1)
            
            loss = criterion(output, tgt_output)
            total_loss += loss.item()
    
    return total_loss / len(dataloader)


def train(num_epochs=30, batch_size=32, num_samples=10000):
    """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""

    # è®°å½• loss å†å²
    train_losses = []  # æ–°å¢
    val_losses = []    # æ–°å¢
        
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“¦ åŠ è½½æ•°æ®...")
    train_loader, val_loader, src_vocab, tgt_vocab = get_dataloaders(
        batch_size=batch_size, 
        num_samples=num_samples
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = Transformer(
        src_vocab_size=len(src_vocab),
        tgt_vocab_size=len(tgt_vocab),
        d_model=256,        # å‡å°æ¨¡å‹ä»¥åŠ å¿«è®­ç»ƒ
        num_heads=8,
        num_layers=3,       # å‡å°‘å±‚æ•°
        d_ff=512,           # å‡å° FFN
        dropout=0.1,
        max_seq_len=100
    ).to(device)
    
    print(f"âœ… æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥ <pad>
    
    # è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {num_epochs} epochs...")
    print("=" * 60)
    
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        start_time = time.time()
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        
        # è¯„ä¼°
        val_loss = evaluate(model, val_loader, criterion, device)
        
        epoch_time = time.time() - start_time
        
        # è®°å½• loss
        train_losses.append(train_loss)  # æ–°å¢
        val_losses.append(val_loss)      # æ–°å¢
        
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Val Loss:   {val_loss:.4f}")
        print(f"  Time:       {epoch_time:.2f}s")
        print("-" * 60)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'src_vocab': src_vocab,
                'tgt_vocab': tgt_vocab,
            }, 'best_model.pt')
            print("ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    
    # ç»˜åˆ¶ loss æ›²çº¿
    print("\nğŸ“Š ç»˜åˆ¶ loss æ›²çº¿...")
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss', marker='o')
    plt.plot(range(1, num_epochs+1), val_losses, label='Val Loss', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig('loss_curve.png', dpi=150, bbox_inches='tight')
    print("âœ… loss æ›²çº¿å·²ä¿å­˜åˆ° loss_curve.png")
    plt.show()
    
    return model, src_vocab, tgt_vocab


if __name__ == "__main__":
    model, src_vocab, tgt_vocab = train(
        num_epochs=30,
        batch_size=32,
        num_samples=10000
    )